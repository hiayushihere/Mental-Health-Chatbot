{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5iJDNkp0Gjs",
        "outputId": "092ee425-87ad-4a1e-91ee-3d2f73fb9ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRp8tIXe0NDt",
        "outputId": "11be089e-2ada-445c-9b6c-b4869eb5c176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=8f2b9b8543dcb862349c9f0fdd54c9f75ae8039f67710d69172f1c387d66c839\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: xxhash, fsspec, dill, rouge_score, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 fsspec-2025.3.0 multiprocess-0.70.16 rouge_score-0.1.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score datasets evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TdYQnQK0Qxr",
        "outputId": "00815834-3d1f-407a-c4bd-6b912a702b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q evaluate bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtcKOLjN0RWa",
        "outputId": "e71b2ff3-ba17-477c-8921-4525225e7c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain faiss-cpu sentence-transformers transformers accelerate bitsandbytes peft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k6VRXrs0T8U",
        "outputId": "7193a884-fdf2-49c8-fdb0-fcf50dae519a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/437.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-core langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iLXzsiN10WGl"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
        "from google.colab import files\n",
        "from huggingface_hub import login\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import default_data_collator\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "import math\n",
        "import re\n",
        "import math\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import os\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import seaborn as sns\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZEWN6-40bgq",
        "outputId": "b2b4b6c0-d74e-40da-fca2-bfb3473842ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGDINeXc0dfR"
      },
      "outputs": [],
      "source": [
        "login(token=\"hf_SAXsDuZzidPssAeMEkRuUWZPaddtoken\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmhDhRlX0gOR",
        "outputId": "99ee854c-b670-44ed-a76e-dae3b1eab899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/303.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aJVW1eVO0ivu"
      },
      "outputs": [],
      "source": [
        "# ---- Step 1: Initialize the embedding model ----\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# ---- List your file paths correctly ----\n",
        "pdf_paths = [\n",
        "    \"/content/drive/MyDrive/The_Stress_Management.pdf\",\n",
        "    \"/content/drive/MyDrive/overcomingdepressionworkbook.pdf\"\n",
        "]\n",
        "\n",
        "# ---- Function to load and split PDFs ----\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def load_and_split_docs(pdf_paths):\n",
        "    all_docs = []\n",
        "    for file_path in pdf_paths:\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        all_docs.extend(loader.load())\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    chunks = text_splitter.split_documents(all_docs)\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CBHBlLIgdE9L"
      },
      "outputs": [],
      "source": [
        "def setup_vectorstore(pdf_paths):\n",
        "    chunks = load_and_split_docs(pdf_paths)\n",
        "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uskyfboi0lDW"
      },
      "outputs": [],
      "source": [
        "def load_rag_pipeline():\n",
        "    # Changed function name to reflect what it actually does\n",
        "    model_id = \"declare-lab/flan-alpaca-base\"  # Using this model for RAG responses\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    gen_pipeline = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "    return gen_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CmLlarNB0oLH"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "def setup_rag_pipeline(pdf_paths):\n",
        "    chunks = load_and_split_docs(pdf_paths)\n",
        "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    llm_pipeline = load_rag_pipeline()  # Raw HF pipeline\n",
        "    return retriever, llm_pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "SwaRphhi0qvK"
      },
      "outputs": [],
      "source": [
        "def ask_question(question, retriever, gen_pipeline, k=3):\n",
        "    docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs[:k])\n",
        "\n",
        "    prompt = f\"\"\"Use the context to answer the question concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Only return the answer. Do not include the context or restate the question.\n",
        "\"\"\"\n",
        "\n",
        "    # Call the raw pipeline correctly\n",
        "    output = gen_pipeline(prompt)[0][\"generated_text\"]\n",
        "\n",
        "    # Extract only the generated answer part\n",
        "    answer = output.replace(prompt, \"\").strip()\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ba7xB-90tVD",
        "outputId": "e43b8d22-61dc-4001-a57d-daadcd663075"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 19 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 59 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 76 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 266 0 (offset 0)\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Regular exercise can help you overcome your anxiety in a number of ways. Exercise can increase the production of chemicals that can li your mood and regulate your emoons. Exercise can also increase the oxygen flow to your help you heal your beliefs, keep a positive mood and mindset, and build your infrastructure.\n"
          ]
        }
      ],
      "source": [
        "retriever, gen_pipeline = setup_rag_pipeline(pdf_paths)\n",
        "answer = ask_question(\"how do I deal with extreme anxiety?\", retriever, gen_pipeline)\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO9f0a_RXYx_",
        "outputId": "f1ddb312-8e2f-4398-8a44-38db0fcca48c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Model extracted to: /content/drive/MyDrive/mistral-lora-finetuned2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "zip_path = \"/content/drive/MyDrive/mistral-lora-finetuned.zip\"\n",
        "extract_path = \"/content/drive/MyDrive/mistral-lora-finetuned2\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\" Model extracted to:\", extract_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "7e0d980c05784491bb78bcfce247c396",
            "50b3ca5d5eb94bd3b0006cc8842ae2c2",
            "843042e1081841e99a21a1b6b9f41500",
            "a460fdcffbc745658b5535e00cbb98da",
            "da705273c9674a68a1b9ec01209525f3",
            "a9e9fd2b20bb453da664596a9d4c14a1",
            "97773050a9584b4083daa59e6a80fb06",
            "b794a5e570ee4301ab6918d3af61b02f",
            "39b1fe573a984228ada25cc098a47c1f",
            "b0037615543f47ccb02a11a98c725e98",
            "7035de74ece24873b5f16590afa54d46"
          ]
        },
        "id": "LSayb4iS0wLE",
        "outputId": "fd1e0cff-9889-46fd-b3c3-d6a49b2744f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading tokenizer...\n",
            "Loading base model with 4-bit quantization and disk offload...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e0d980c05784491bb78bcfce247c396",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- ⚙️ BitsAndBytes Config with CPU + Disk Offload ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "# --- 📁 Paths ---\n",
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "adapter_path = \"/content/drive/MyDrive/mistral-lora-finetuned2/mistral-lora-finetuned\"\n",
        "offload_dir = \"/content/model_offload\"\n",
        "\n",
        "# --- 🔤 Load Tokenizer ---\n",
        "print(\" Loading tokenizer...\")\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
        "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
        "\n",
        "# --- 🧠 Load Base Model with Quantization + Offload ---\n",
        "print(\"Loading base model with 4-bit quantization and disk offload...\")\n",
        "mistral_base = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": \"cuda:0\"},\n",
        "    offload_folder=offload_dir,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# --- 🔁 Load LoRA Adapter ---\n",
        "print(\"Loading fine-tuned LoRA adapter...\")\n",
        "mistral_model = PeftModel.from_pretrained(mistral_base, adapter_path)\n",
        "mistral_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41Rp37NJ03TK"
      },
      "outputs": [],
      "source": [
        "def compute_perplexity(model, tokenizer, dataset, max_length=512):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for example in dataset:\n",
        "            input_ids = tokenizer.encode(example['text'], return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, labels=input_ids)\n",
        "                loss = outputs.loss\n",
        "                losses.append(loss.item())\n",
        "    return math.exp(sum(losses) / len(losses))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxJ4QlOO05wL"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "def compute_distinct_n(generations, n=1):\n",
        "    n_grams = Counter()\n",
        "    total = 0\n",
        "    for gen in generations:\n",
        "        tokens = gen.split()\n",
        "        total += max(len(tokens) - n + 1, 0)\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            n_gram = tuple(tokens[i:i+n])\n",
        "            n_grams[n_gram] += 1\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    return len(n_grams) / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZRXMUMM07xn"
      },
      "outputs": [],
      "source": [
        "def generate_outputs(model, tokenizer, prompts, max_new_tokens=100):\n",
        "    model.eval()\n",
        "    generations = []\n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generations.append(response)\n",
        "    return generations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zG7VDBh0-PL"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, prompts):\n",
        "    print(\" Generating responses...\")\n",
        "    generations = generate_outputs(model, tokenizer, prompts)\n",
        "\n",
        "    print(\" Calculating diversity...\")\n",
        "    distinct_1 = compute_distinct_n(generations, n=1)\n",
        "    distinct_2 = compute_distinct_n(generations, n=2)\n",
        "\n",
        "    print(\" Calculating perplexity...\")\n",
        "    # Wrap prompts as dataset objects for perplexity function\n",
        "    dataset = [{'text': p} for p in prompts]\n",
        "    perplexity = compute_perplexity(model, tokenizer, dataset)\n",
        "\n",
        "    return {\n",
        "        \"Perplexity\": perplexity,\n",
        "        \"Distinct-1\": distinct_1,\n",
        "        \"Distinct-2\": distinct_2\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLgZ-jBb1Asc"
      },
      "outputs": [],
      "source": [
        "sample_prompts = [\n",
        "    \"I'm feeling very anxious about my upcoming exams.\",\n",
        "    \"I feel like nobody understands what I'm going through.\",\n",
        "    \"What should I do when I'm overwhelmed with studies?\"\n",
        "]\n",
        "results = evaluate_model(mistral_model, mistral_tokenizer, sample_prompts)\n",
        "print(\"\\n Evaluation Results:\\n\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8Ix__DGeZvV"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define checkpoint path\n",
        "checkpoint_path = \"/content/drive/MyDrive/checkpoint-852\"\n",
        "\n",
        "# Step 2: Load the model and tokenizer\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(checkpoint_path)\n",
        "roberta_model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device here\n",
        "roberta_model.to(device) # Move the model to the defined device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTXCQ53jZu1K"
      },
      "outputs": [],
      "source": [
        "similarity_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q-_Dutpq8IZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import random\n",
        "\n",
        "# Track previously given suggestions and questions to avoid repetition\n",
        "previous_suggestions = set()\n",
        "previous_questions = set()\n",
        "\n",
        "# Function to classify text for stress using RoBERTa\n",
        "def classify_stress(text, model, tokenizer, device):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "        confidence = torch.softmax(logits, dim=1)[0][predicted_class].item()\n",
        "    return (\"stressed\" if predicted_class == 1 else \"not stressed\", confidence)\n",
        "\n",
        "# Function to identify stress type using Mistral\n",
        "def identify_stress_type(text, model, tokenizer):\n",
        "    prompt = f\"\"\"Analyze this message and identify the specific type of stress or emotional issue the person is experiencing.\n",
        "Choose ONE specific category from these options: academic stress, work stress, relationship problems, family issues, health anxiety, financial stress, social anxiety, grief, identity issues, or general anxiety.\n",
        "\n",
        "Guidelines:\n",
        "- If the message mentions loneliness, lack of friends, social isolation, or feeling alone, prioritize **social anxiety**.\n",
        "- If the message mentions romantic or interpersonal conflicts (e.g., partner, breakup), prioritize **relationship problems**.\n",
        "- If the message mentions loss, mourning, or death, prioritize **grief**.\n",
        "- If the message mentions school or studies, prioritize **academic stress**.\n",
        "- If the message mentions job or workplace, prioritize **work stress**.\n",
        "- If no specific category fits, default to **general anxiety**.\n",
        "- Focus on keywords like 'friends,' 'alone,' 'partner,' 'work,' 'school,' etc., to make the choice.\n",
        "\n",
        "Message: {text}\n",
        "\n",
        "The primary type of stress appears to be:\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    stress_type = response.replace(prompt, \"\").strip().split('.')[0].split('\\n')[0].strip().lower()\n",
        "    loneliness_keywords = [\"lonely\", \"alone\", \"no friends\", \"isolated\"]\n",
        "    if any(keyword in text.lower() for keyword in loneliness_keywords):\n",
        "        stress_type = \"social anxiety\"\n",
        "    return stress_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kSG9n8vrBvg"
      },
      "outputs": [],
      "source": [
        "# Function to clean Mistral responses\n",
        "def clean_response(text):\n",
        "    text = re.sub(r'^(User|Bot|Assistant|Therapist):\\s*', '', text)\n",
        "    text = re.sub(r'Conversation (history|so far):.+?Your (next supportive message|response):', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'The following is a supportive conversation.+?Your compassionate response:', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'Your response should be.+?:', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'\\[/?INST\\]', '', text)\n",
        "    text = re.sub(r'[\"\"]', '', text)\n",
        "    text = re.sub(r'^\\d+\\.\\s*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'Exercise \\d+\\.\\d+:.*?(?=\\n|$)', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'•\\s+', '', text)\n",
        "    text = re.sub(r'- ', '', text)\n",
        "    text = re.sub(r'\\* ', '', text)\n",
        "    text = re.sub(r'Set a timer for fifteen minutes.*?think too much\\.', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'On a sheet of paper.*?want to do\\.', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'[Ss]tress [Rr]elease [Jj]ournal.*?(?=\\n|$)', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXhiketcrFlE"
      },
      "outputs": [],
      "source": [
        "# Function to get Mistral response\n",
        "def get_mistral_response(prompt, model, tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=250,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.92,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    if response.startswith(prompt):\n",
        "        response = response[len(prompt):].strip()\n",
        "    return clean_response(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S2qWAycrQWY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to generate RAG suggestions\n",
        "def get_rag_suggestion(query, stress_type, retriever, gen_pipeline, mistral_context=None):\n",
        "    global previous_suggestions\n",
        "    enhanced_query = f\"{query} {stress_type} practical coping strategies advice\"\n",
        "    docs = retriever.get_relevant_documents(enhanced_query)\n",
        "    contexts = [doc.page_content for doc in docs[:3]] if docs else [\"\"]\n",
        "    for context in contexts:\n",
        "        prompt = f\"\"\"\n",
        "        You are a mental health assistant helping someone experiencing {stress_type}.\n",
        "\n",
        "        Based on the mental health resource below, their message — \"{query}\" — and the emotional context from a therapist's response — \"{mistral_context or 'No specific emotional context provided'}\" — provide a helpful, non-generic suggestion.\n",
        "\n",
        "        Resource:\n",
        "        {context}\n",
        "\n",
        "        Instructions:\n",
        "        - Address the user directly using **\"you\"** language (e.g., \"You can try...\")\n",
        "        - Provide **one specific, practical suggestion** they can try immediately\n",
        "        - Ensure it is **highly relevant to {stress_type}** and their situation (e.g., '{query}')\n",
        "        - Complement the empathetic tone of the therapist's response\n",
        "        - Focus on strategies for the user, not others\n",
        "        - Avoid generic advice like \"journal\", \"exercise\", \"meditate\", \"sleep\", or \"talk to someone\"\n",
        "        - Avoid assuming relationships (e.g., partner, friends) unless mentioned\n",
        "        - Do **not repeat** previous suggestions\n",
        "        - Provide 2-3 sentences for clarity and detail\n",
        "        - Keep the tone empathetic and encouraging\n",
        "        - Example: For social anxiety, \"You can join an online forum like Reddit for a hobby you enjoy to connect with others at your own pace. This can feel less overwhelming than in-person interactions.\"\n",
        "\n",
        "        Respond with the suggestion only, without intro or closing.\n",
        "        \"\"\"\n",
        "        output = gen_pipeline(\n",
        "            prompt,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.95,\n",
        "            max_new_tokens=120\n",
        "        )[0][\"generated_text\"].strip()\n",
        "        if output.startswith(prompt):\n",
        "            output = output[len(prompt):].strip()\n",
        "        clean_output = re.sub(r'^(Suggestion|Here is a suggestion|A practical suggestion):', '', output)\n",
        "        clean_output = re.sub(r'^\\d+\\.\\s*', '', clean_output, flags=re.MULTILINE)\n",
        "        clean_output = re.sub(r'Exercise \\d+\\.\\d+:.*?•', '', clean_output, flags=re.DOTALL)\n",
        "        clean_output = re.sub(r'•\\s+', '', clean_output)\n",
        "        output_fingerprint = re.sub(r'\\W+', '', clean_output.lower())[:50]\n",
        "        if output_fingerprint not in previous_suggestions and \"journal\" not in output_fingerprint:\n",
        "            previous_suggestions.add(output_fingerprint)\n",
        "            suggestion = f\"Here's a suggestion for you: {clean_output.strip()}\"\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', suggestion)\n",
        "            if len(sentences) > 4:\n",
        "                suggestion = ' '.join(sentences[:4])\n",
        "            return suggestion\n",
        "    return f\"Here's a suggestion for you: You can try a grounding exercise by naming five things you see, four you touch, and three you hear to ease {stress_type}. This can help anchor you when feeling overwhelmed.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGS6bcJDrTUO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to generate follow-up questions\n",
        "def generate_follow_up_question(user_input, conversation_history, stress_type, mistral_model, mistral_tokenizer):\n",
        "    global previous_questions\n",
        "    recent_exchanges = conversation_history[-min(6, len(conversation_history)):]\n",
        "    recent_context = \"\\n\".join(recent_exchanges)\n",
        "    significant_keywords = [\"lonely\", \"alone\", \"anxious\", \"depressed\", \"trauma\", \"bully\", \"grief\", \"introvert\"]\n",
        "    has_significant_details = any(keyword in user_input.lower() for keyword in significant_keywords)\n",
        "    if stress_type and has_significant_details:\n",
        "        prompt = f\"\"\"Based on this conversation about {stress_type}:\n",
        "\n",
        "{recent_context}\n",
        "\n",
        "Generate ONE thoughtful, open-ended question that:\n",
        "1. Shows genuine curiosity about their feelings regarding {stress_type}\n",
        "2. References a specific detail from their latest message: '{user_input}'\n",
        "3. Helps them explore their emotional experience deeper\n",
        "4. Feels natural, empathetic, and flows from the conversation\n",
        "5. Is different from any questions already asked\n",
        "6. Is brief (10-15 words max)\n",
        "\n",
        "Your question (brief, caring, and conversational):\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"Based on this friendly conversation:\n",
        "\n",
        "{recent_context}\n",
        "\n",
        "Generate ONE thoughtful question that:\n",
        "1. Shows genuine interest in their latest message: '{user_input}'\n",
        "2. Keeps the conversation flowing naturally\n",
        "3. Feels warm, engaging, and relevant\n",
        "4. Is specific to what they've mentioned\n",
        "5. Is different from any questions already asked\n",
        "6. Is brief (10-15 words max)\n",
        "\n",
        "Your question (brief, friendly, and conversational):\"\"\"\n",
        "    max_attempts = 3\n",
        "    for _ in range(max_attempts):\n",
        "        question = get_mistral_response(prompt, mistral_model, mistral_tokenizer)\n",
        "        question = re.sub(r'^(Question|Here\\'s a question|My question):', '', question).strip()\n",
        "        question_embedding = similarity_model.encode(question, convert_to_tensor=True)\n",
        "        is_unique = True\n",
        "        for prev_question in previous_questions:\n",
        "            prev_embedding = similarity_model.encode(prev_question, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(question_embedding, prev_embedding).item()\n",
        "            if similarity > 0.8:\n",
        "                is_unique = False\n",
        "                break\n",
        "        if is_unique:\n",
        "            previous_questions.add(question)\n",
        "            return question\n",
        "    backup_questions = [\n",
        "        \"What helps you feel a bit better when you’re overwhelmed?\",\n",
        "        \"How do these feelings affect your daily routine?\",\n",
        "        \"What small step could you try today to feel supported?\",\n",
        "        \"What’s been the toughest part of this for you?\"\n",
        "    ] if stress_type else [\n",
        "        \"What’s something you’ve been thinking about lately?\",\n",
        "        \"What do you enjoy doing to unwind?\",\n",
        "        \"How’s your day been so far?\",\n",
        "        \"What would make today feel a bit brighter?\"\n",
        "    ]\n",
        "    for _ in range(len(backup_questions)):\n",
        "        backup = random.choice(backup_questions)\n",
        "        if backup not in previous_questions:\n",
        "            previous_questions.add(backup)\n",
        "            return backup\n",
        "    return \"What else would you like to share today?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiZG_Gu8mhqC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Main chatbot function\n",
        "def run_mental_health_chatbot(roberta_model, roberta_tokenizer, mistral_model, mistral_tokenizer, retriever, gen_pipeline):\n",
        "    print(\"\\n💙 Welcome to the Mental Health Support Chatbot! How are you feeling today?  Type 'exit' or 'stop' to end the conversation.\\n\")\n",
        "    conversation_history = []\n",
        "    device = next(roberta_model.parameters()).device\n",
        "    stress_type = None\n",
        "    is_stressed = False\n",
        "    bot_name = \"Mia\"\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() in [\"exit\", \"stop\", \"quit\"]:\n",
        "            print(f\"\\n🧚🏻‍♂️ {bot_name}: Thank you for chatting with me. Take care and be gentle with yourself. I'm here whenever you need me!\")\n",
        "            break\n",
        "        conversation_history.append(f\"You: {user_input}\")\n",
        "        # Detect if user is requesting advice\n",
        "        advice_keywords = [\"suggest\", \"help\", \"what should i do\", \"advice\", \"recommend\"]\n",
        "        is_advice_request = any(keyword in user_input.lower() for keyword in advice_keywords)\n",
        "        if len(conversation_history) == 1:\n",
        "            stress_status, confidence = classify_stress(user_input, roberta_model, roberta_tokenizer, device)\n",
        "            is_stressed = (stress_status == \"stressed\")\n",
        "            confidence_pct = f\"{confidence:.2f}\"\n",
        "            if is_stressed:\n",
        "                stress_type = identify_stress_type(user_input, mistral_model, mistral_tokenizer)\n",
        "                stress_info = f\"[Detected: {stress_status.capitalize()} - {stress_type} (Confidence: {confidence_pct})]\\n\"\n",
        "                prompt = f\"\"\"As {bot_name}, a compassionate therapist trained on empathetic dialogues, respond to someone experiencing {stress_type}.\n",
        "The person said: \"{user_input}\"\n",
        "\n",
        "Your response should:\n",
        "1. Express deep empathy and understanding, reflecting a caring, supportive tone\n",
        "2. Acknowledge their {stress_type} and connect to specific details (e.g., '{user_input}')\n",
        "3. Reference SPECIFIC emotions (e.g., 'depressed') and circumstances (e.g., 'no friends')\n",
        "4. Share 2-3 insights that validate their experience and show emotional connection\n",
        "5. Be warm, conversational, and grounded in their input, avoiding assumptions\n",
        "6. Avoid referencing unmentioned events or resources\n",
        "7. Avoid questions; focus on validation and support\n",
        "8. Keep the response 4-6 sentences for depth\n",
        "\n",
        "Your empathetic response:\"\"\"\n",
        "                mistral_response = get_mistral_response(prompt, mistral_model, mistral_tokenizer)\n",
        "                if is_advice_request:\n",
        "                    advice_prompt = f\"\"\"As {bot_name}, provide a specific, practical suggestion for someone experiencing {stress_type}.\n",
        "The person said: \"{user_input}\"\n",
        "\n",
        "Your suggestion should:\n",
        "1. Address the user directly with **\"you\"** language\n",
        "2. Be highly relevant to {stress_type} and their situation (e.g., '{user_input}')\n",
        "3. Offer one actionable step they can try immediately\n",
        "4. Avoid generic advice like \"journal\", \"exercise\", \"meditate\", \"sleep\", \"talk to someone\"\n",
        "5. Avoid assuming relationships unless mentioned\n",
        "6. Be empathetic and encouraging\n",
        "7. Provide 2-3 sentences for clarity\n",
        "\n",
        "Your suggestion:\"\"\"\n",
        "                    mistral_suggestion = get_mistral_response(advice_prompt, mistral_model, mistral_tokenizer)\n",
        "                    suggestion = f\"Here's a suggestion for you: {mistral_suggestion.strip()}\"\n",
        "                else:\n",
        "                    suggestion = get_rag_suggestion(user_input, stress_type, retriever, gen_pipeline, mistral_context=mistral_response)\n",
        "                follow_up = generate_follow_up_question(user_input, conversation_history, stress_type, mistral_model, mistral_tokenizer)\n",
        "                final_response = f\"{stress_info}\\n{mistral_response}\\n\\n{suggestion}\\n\\n{follow_up}\"\n",
        "            else:\n",
        "                stress_info = f\"[Detected: Not stressed (Confidence: {confidence_pct})]\\n\"\n",
        "                prompt = f\"\"\"As {bot_name}, a friendly conversational partner trained on empathetic dialogues, respond to: \"{user_input}\"\n",
        "\n",
        "Your response should:\n",
        "1. Be warm, positive, and uplifting, reflecting a caring tone\n",
        "2. Connect with specific details they've shared (e.g., '{user_input}')\n",
        "3. Feel natural, conversational, and engaging\n",
        "4. Show authentic interest and emotional connection\n",
        "5. Avoid referencing unmentioned events or resources\n",
        "6. Avoid questions; focus on acknowledgment\n",
        "7. Keep the response 4-6 sentences for depth\n",
        "\n",
        "Your friendly response:\"\"\"\n",
        "                mistral_response = get_mistral_response(prompt, mistral_model, mistral_tokenizer)\n",
        "                follow_up = generate_follow_up_question(user_input, conversation_history, None, mistral_model, mistral_tokenizer)\n",
        "                final_response = f\"{stress_info}\\n{mistral_response}\\n\\n{follow_up}\"\n",
        "        else:\n",
        "            recent_history = conversation_history[-min(8, len(conversation_history)):]\n",
        "            history_snippet = \"\\n\".join(recent_history)\n",
        "            if is_stressed and stress_type:\n",
        "                prompt = f\"\"\"As {bot_name}, continue this supportive conversation with someone experiencing {stress_type}.\n",
        "\n",
        "Previous conversation:\n",
        "{history_snippet}\n",
        "\n",
        "Their latest message: \"{user_input}\"\n",
        "\n",
        "Respond as {bot_name}, a compassionate therapist who:\n",
        "1. References specific details from the conversation (e.g., emotions, events)\n",
        "2. Acknowledges new information in their latest message (e.g., '{user_input}')\n",
        "3. Offers 2-3 insights tailored to their experience, showing deep understanding\n",
        "4. Maintains a warm, empathetic tone, leveraging empathetic training\n",
        "5. Avoids generic responses and assumptions about unmentioned events\n",
        "6. Avoids questions unless necessary to clarify\n",
        "7. Keeps the response 4-6 sentences for depth\n",
        "\n",
        "Your empathetic response:\"\"\"\n",
        "                mistral_response = get_mistral_response(prompt, mistral_model, mistral_tokenizer)\n",
        "                if is_advice_request:\n",
        "                    advice_prompt = f\"\"\"As {bot_name}, provide a specific, practical suggestion for someone experiencing {stress_type}.\n",
        "The person said: \"{user_input}\"\n",
        "\n",
        "Your suggestion should:\n",
        "1. Address the user directly with **\"you\"** language\n",
        "2. Be highly relevant to {stress_type} and their situation (e.g., '{user_input}')\n",
        "3. Offer one actionable step they can try immediately\n",
        "4. Avoid generic advice like \"journal\", \"exercise\", \"meditate\", \"sleep\", \"talk to someone\"\n",
        "5. Avoid assuming relationships unless mentioned\n",
        "6. Be empathetic and encouraging\n",
        "7. Provide 2-3 sentences for clarity\n",
        "\n",
        "Your suggestion:\"\"\"\n",
        "                    mistral_suggestion = get_mistral_response(advice_prompt, mistral_model, mistral_tokenizer)\n",
        "                    suggestion = f\"Here's a suggestion for you: {mistral_suggestion.strip()}\"\n",
        "                else:\n",
        "                    suggestion = get_rag_suggestion(user_input, stress_type, retriever, gen_pipeline, mistral_context=mistral_response)\n",
        "                follow_up = generate_follow_up_question(user_input, conversation_history, stress_type, mistral_model, mistral_tokenizer)\n",
        "                final_response = f\"{mistral_response}\\n\\n{suggestion}\\n\\n{follow_up}\"\n",
        "            else:\n",
        "                prompt = f\"\"\"As {bot_name}, continue this friendly conversation:\n",
        "\n",
        "Previous conversation:\n",
        "{history_snippet}\n",
        "\n",
        "Their latest message: \"{user_input}\"\n",
        "\n",
        "Respond as {bot_name} with:\n",
        "1. A natural, conversational tone\n",
        "2. Connection to specific details they just shared (e.g., '{user_input}')\n",
        "3. References to earlier conversation for continuity\n",
        "4. Warmth, authentic interest, and emotional connection\n",
        "5. Avoid assumptions about unmentioned events\n",
        "6. Avoid questions unless necessary\n",
        "7. Keep the response 4-6 sentences for depth\n",
        "\n",
        "Your engaging response:\"\"\"\n",
        "                mistral_response = get_mistral_response(prompt, mistral_model, mistral_tokenizer)\n",
        "                follow_up = generate_follow_up_question(user_input, conversation_history, None, mistral_model, mistral_tokenizer)\n",
        "                final_response = f\"{mistral_response}\\n\\n{follow_up}\"\n",
        "        print(f\"\\n🧚🏻 {bot_name}: {final_response}\")\n",
        "        conversation_history.append(f\"{bot_name}: {final_response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_mental_health_chatbot(\n",
        "        roberta_model,\n",
        "        roberta_tokenizer,\n",
        "        mistral_model,\n",
        "        mistral_tokenizer,\n",
        "        retriever,\n",
        "        gen_pipeline\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39b1fe573a984228ada25cc098a47c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50b3ca5d5eb94bd3b0006cc8842ae2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9e9fd2b20bb453da664596a9d4c14a1",
            "placeholder": "​",
            "style": "IPY_MODEL_97773050a9584b4083daa59e6a80fb06",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "7035de74ece24873b5f16590afa54d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e0d980c05784491bb78bcfce247c396": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50b3ca5d5eb94bd3b0006cc8842ae2c2",
              "IPY_MODEL_843042e1081841e99a21a1b6b9f41500",
              "IPY_MODEL_a460fdcffbc745658b5535e00cbb98da"
            ],
            "layout": "IPY_MODEL_da705273c9674a68a1b9ec01209525f3"
          }
        },
        "843042e1081841e99a21a1b6b9f41500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b794a5e570ee4301ab6918d3af61b02f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39b1fe573a984228ada25cc098a47c1f",
            "value": 1
          }
        },
        "97773050a9584b4083daa59e6a80fb06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a460fdcffbc745658b5535e00cbb98da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0037615543f47ccb02a11a98c725e98",
            "placeholder": "​",
            "style": "IPY_MODEL_7035de74ece24873b5f16590afa54d46",
            "value": " 1/2 [00:46&lt;00:46, 46.20s/it]"
          }
        },
        "a9e9fd2b20bb453da664596a9d4c14a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0037615543f47ccb02a11a98c725e98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b794a5e570ee4301ab6918d3af61b02f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da705273c9674a68a1b9ec01209525f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
